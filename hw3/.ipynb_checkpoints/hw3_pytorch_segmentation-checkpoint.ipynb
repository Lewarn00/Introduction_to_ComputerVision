{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pa4Z_RwvQOxd"
   },
   "source": [
    "# Instructions\n",
    "\n",
    "In this section, you can experiment and design your own CNN architecture\n",
    "for segmentation. Here, it is your job to experiment with architectures, hyperparameters,\n",
    "loss functions, and optimizers to train a model.\n",
    "\n",
    "The task consists of performing pixel-wise classification on a synthetic dataset\n",
    "of 32x32 RGB images. Each image was generated by placing a MNIST sample (a grayscale\n",
    "image of a digit between 0 and 9) on top of a CIFAR-10 sample (a RGB image drawn from\n",
    "one of 10 possible classes). Each image has an accompanying target tensor of size 32x32,\n",
    "where in each pixel location (i,j) it contains the ground-truth label of the MNIST digit\n",
    "(ranging from 0 to 9) or of the CIFAR-10 image (ranging from 10 to 19), depending on whether\n",
    "the (i,j) pixel in the original image belongs to the superposed MNIST image or not. The\n",
    "metric of interest here is pixel-wise accuracy, which is the fraction of pixels in each image\n",
    "for which your model predicted the correct class (out of a total of 20 classes, as described \n",
    "above).\n",
    "\n",
    "Note that there are many ways to frame the above task. For example, your CNN can directly\n",
    "output a 20x32x32 tensor for each input image, representing a distribution over the possible\n",
    "20 classes for each of the 32x32 pixels. However, note that the problem has a lot of additional\n",
    "structure: for example, each 32x32 target tensor only has two distinct numbers in it, the label\n",
    "of the MNIST digit and the label of the CIFAR-10 background image -- accounting for such\n",
    "structure will make training faster and likely improve your model's final performance. Your\n",
    "model should be able to achieve at least 78% accuracy on the test set when trained for 100 epochs.\n",
    "\n",
    "It is highly suggested to use google colab and run the notebook on a GPU node.\n",
    "Check https://colab.research.google.com/ and look for tutorials online on how to use it.\n",
    "To use a GPU go to Runtime -> Change runtime type and select GPU. It is unlikely that you \n",
    "will be able to achieve a reasonable performance if you train your model on CPU.\n",
    "\n",
    "You must complete the test and train functions below.\n",
    "\n",
    "You can use either nn.Module or nn.Sequential API during model design.\n",
    "- Layers in torch.nn package: http://pytorch.org/docs/stable/nn.html\n",
    "- Activations: http://pytorch.org/docs/stable/nn.html#non-linear-activations\n",
    "- Loss functions: http://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "- Optimizers: http://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "To finish this section step by step, you need to:\n",
    "\n",
    "(1) Prepare data by building dataset and dataloader. (alreadly provided below)\n",
    "\n",
    "(2) Implement training code (6 points) & testing code (6 points) including saving and loading of models.\n",
    "\n",
    "(3) Construct a model (12 points) and choose an optimizer (3 points).\n",
    "\n",
    "(4) Describe what you did, any additional features that you implemented, and/or any graphs that you made in the process of training and evaluating your network.  Report final test accuracy @100 epochs in a writeup: hw3.pdf (3 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ZjnJPFe1xB7",
    "outputId": "5d2120fc-653a-4003-f37f-e161031bf599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/hw3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "print(os.getcwd())\n",
    "os.chdir(\"/content/drive/MyDrive/hw3\")\n",
    "from utils import SegDataset\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "torch.set_printoptions(threshold=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Li9kIQ68keX",
    "outputId": "e51a039c-5f90-4888-deff-d871e3538ebd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mxrv-F6V5dkl"
   },
   "source": [
    "## Data Preparation (NO need to modify):\n",
    "\n",
    "We set up a Dataset object for each split (train / test); Datasets\n",
    "load training examples one at a time, so we wrap each Dataset in a\n",
    "DataLoader which iterates through the Dataset and forms minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "id": "qZXaLONN1xB9"
   },
   "outputs": [],
   "source": [
    "seg_train = SegDataset('./data', train=True, transform=None)\n",
    "loader_train = DataLoader(seg_train, batch_size=64, shuffle=True)\n",
    "seg_test = SegDataset('./data', train=False, transform=None)\n",
    "loader_test = DataLoader(seg_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhQErUjgTri-"
   },
   "source": [
    "Now we will see how a sample looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "qY5MDlKn1xB-",
    "outputId": "aac43bae-d773-419f-ae51-5bd8dc5c14cb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=256x256 at 0x7F88062B77D0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [15, 15, 15, 15, 15, 15,  3,  3,  3,  3, 15, 15, 15, 15, 15, 15],\n",
      "        [15, 15, 15, 15, 15,  3, 15, 15, 15, 15,  3, 15, 15, 15, 15, 15],\n",
      "        [15, 15, 15, 15, 15, 15, 15, 15,  3,  3, 15, 15, 15, 15, 15, 15],\n",
      "        [15, 15, 15, 15, 15,  3,  3,  3,  3, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [15, 15, 15, 15, 15,  3,  3, 15,  3,  3, 15, 15, 15, 15, 15, 15],\n",
      "        [15, 15, 15, 15, 15, 15, 15, 15, 15,  3, 15, 15, 15, 15, 15, 15],\n",
      "        [15, 15, 15, 15, 15, 15, 15, 15, 15,  3, 15, 15, 15, 15, 15, 15],\n",
      "        [15, 15, 15, 15, 15,  3,  3,  3,  3,  3, 15, 15, 15, 15, 15, 15],\n",
      "        [15, 15, 15, 15,  3, 15, 15,  3,  3,  3, 15, 15, 15, 15, 15, 15],\n",
      "        [15, 15, 15, 15,  3,  3,  3,  3, 15, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15]])\n"
     ]
    }
   ],
   "source": [
    "from google.colab.patches import cv2_imshow\n",
    "import cv2\n",
    "\n",
    "def show(x):\n",
    "    img = x.data.cpu().permute(1, 2, 0).numpy() * 255\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (256,256), interpolation = cv2.INTER_AREA)\n",
    "    cv2_imshow(img)\n",
    "\n",
    "data, target = next(iter(loader_train))\n",
    "show(data[0])\n",
    "print(target[0,::2,::2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lW8h9x-VXsf"
   },
   "source": [
    "## Training (6 points)\n",
    "\n",
    "Train a model on the given dataset using the PyTorch Module API.\n",
    "\n",
    "Inputs:\n",
    "- loader_train: The loader from which train samples will be drawn from.\n",
    "- loader_test: The loader from which test samples will be drawn from.\n",
    "- model: A PyTorch Module giving the model to train.\n",
    "- optimizer: An Optimizer object we will use to train the model\n",
    "- epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "\n",
    "Returns: Nothing, but prints model accuracies during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "id": "chToiTtj1xB-"
   },
   "outputs": [],
   "source": [
    "def train(loader_train, loader_test, model, optimizer, epochs=100):\n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            ##########################################################################\n",
    "            x = x.to('cuda:0')\n",
    "            y = y.to('cuda:0')\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ##########################################################################\n",
    "            if t % 100 == 0:\n",
    "                print('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpohLOwXghjv"
   },
   "source": [
    "### Testing (6 points)\n",
    "Test a model using the PyTorch Module API.\n",
    "\n",
    "Inputs:\n",
    "- loader: The loader from which test samples will be drawn from.\n",
    "- model: A PyTorch Module giving the model to test.\n",
    "\n",
    "Returns: Nothing, but prints model accuracies during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "id": "xxwoFc941xB_"
   },
   "outputs": [],
   "source": [
    "def test(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.train() #eval() # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            ##########################################################################\n",
    "            x = x.to('cuda:0')\n",
    "            y = y.to('cuda:0')\n",
    "            y_pred = model(x)\n",
    "            pred = y_pred.argmax(1)\n",
    "            num_samples += pred.numel()\n",
    "            num_correct += (pred == y).float().sum().item()\n",
    "            acc = float(num_correct) / num_samples\n",
    "            ##########################################################################\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElTMvGW85dkw"
   },
   "source": [
    "\n",
    "## Design/choose your own model structure (12 points) and optimizer (3 points).\n",
    "Below are things you may want to try:\n",
    "(1) Model hyperparams:\n",
    "- Filter size, Number of filters and layers. You need to make careful choices to\n",
    "tradeoff the computational efficiency and accuracy (especially in this assignment).\n",
    "- Pooling vs Strided Convolution\n",
    "- Batch normalization\n",
    "\n",
    "(2) New Network architecture:\n",
    "- You can borrow some cool ideas from novel convnets design, such as ResNet where\n",
    "the input from the previous layer is added to the output\n",
    "https://arxiv.org/abs/1512.03385\n",
    "- Note: Don't directly use the existing network design.\n",
    "\n",
    "(3) Different optimizers like SGD, Adam, Adagrad, RMSprop, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "id": "WVvhJ7PT1xCA"
   },
   "outputs": [],
   "source": [
    "#Basic model, feel free to use more complicated ones to fit your model design.\n",
    "\n",
    "class myNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myNet, self).__init__()\n",
    "        self.sequenceUp1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size = 3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(64, affine = False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size = 3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(64, affine = False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size = 3, padding=1, stride=1),\n",
    "            #nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.sequenceUp2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size = 3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(128, affine = False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size = 3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(128, affine = False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size = 3, padding=1, stride=1),\n",
    "            #nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.sequenceUp3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size = 3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(256, affine = False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size = 3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(256, affine = False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size = 3, padding=1, stride=1),\n",
    "            #nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.sequenceUp4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size = 3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(512, affine = False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size = 3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(512, affine = False),\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )   \n",
    "\n",
    "        self.sequenceDown1 = nn.Sequential(           \n",
    "            nn.ConvTranspose2d(512, 256, kernel_size = 3, stride=2, padding=1)\n",
    "        ) \n",
    "        self.sequenceDown2 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size = 3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(256, affine = False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size = 3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(256, affine = False), \n",
    "            nn.ReLU(),           \n",
    "            nn.ConvTranspose2d(256, 128, kernel_size = 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128, affine = False)\n",
    "        )   \n",
    "        self.sequenceDown3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size = 3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(128, affine = False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size = 3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(128, affine = False), \n",
    "            nn.ReLU(),           \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size = 4, padding=1, stride=2),\n",
    "            nn.BatchNorm2d(64, affine = False)\n",
    "        )    \n",
    "        self.sequenceDown4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size = 3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(64, affine = False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size = 3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(64, affine = False), \n",
    "            nn.ReLU(),           \n",
    "            nn.Conv2d(64, 20, kernel_size = 3, padding=1, stride=1),\n",
    "        )     \n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2d_64_64 = nn.Conv2d(64, 64, kernel_size =3, padding=1,stride=1)\n",
    "        self.conv2d_128_128 = nn.Conv2d(128, 128, kernel_size =3, padding=1,stride=1)\n",
    "        self.conv2d_256_256 = nn.Conv2d(256, 256, kernel_size =3, padding=1,stride=1)\n",
    "        self.conv2d_trasnpose_512_256 = nn.ConvTranspose2d(512, 256, kernel_size=4, padding=1,stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        up1 = self.sequenceUp1(x)\n",
    "        up2 = self.sequenceUp2(self.max_pool(up1))\n",
    "        up3 = self.sequenceUp3(self.max_pool(up2))\n",
    "        up4 = self.sequenceUp4(self.max_pool(up3))\n",
    "        temp = self.conv2d_trasnpose_512_256(up4)\n",
    "        concat1 = torch.cat((temp, self.conv2d_256_256(up3)), dim = 1)\n",
    "        down2 = self.sequenceDown2(concat1)\n",
    "        concat2 = torch.cat((down2, self.conv2d_128_128(up2)), dim = 1)\n",
    "        down3 = self.sequenceDown3(concat2)\n",
    "        concat3 = torch.cat((down3, self.conv2d_64_64(up1)), dim = 1)\n",
    "        down4 = self.sequenceDown4(concat3)\n",
    "        return down4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CdZxQ0AnYNB"
   },
   "source": [
    "Describe your design details in the writeup hw3.pdf. (3 points)\n",
    "\n",
    "Finish your model and optimizer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jGUtMqkz1xCA",
    "outputId": "2b8315b3-da16-444d-fbde-cab17b317554"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0, loss = 3.0966\n",
      "Epoch 0, Iteration 100, loss = 1.9740\n",
      "Epoch 0, Iteration 200, loss = 1.7024\n",
      "Epoch 0, Iteration 300, loss = 1.6736\n",
      "Epoch 0, Iteration 400, loss = 1.8104\n",
      "Epoch 0, Iteration 500, loss = 1.5931\n",
      "Epoch 0, Iteration 600, loss = 1.5736\n",
      "Epoch 0, Iteration 700, loss = 1.2898\n",
      "Got 4610875 / 10240000 correct (45.03)\n"
     ]
    }
   ],
   "source": [
    "model = myNet()\n",
    "#optimizer = optim.SGD(model.parameters(), lr, momentum, weight_decay)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "train(loader_train, loader_test, model, optimizer, epochs=1)\n",
    "test(loader_test, model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "hw3_pytorch_segmentation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
